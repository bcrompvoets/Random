{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breannacrompvoets/miniforge3/envs/SF/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "import multiprocess as mp\n",
    "\n",
    "from custom_dataloader import replicate_data\n",
    "from NN_Defs import TwoLayerMLP, BaseMLP, train, validate\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# data load\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\")\n",
    "\n",
    "# CM21 Split\n",
    "amounts_train = [300,300,300,300,27,70,300]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimate(estimator, scoring_func=None, random_seed=0, n_splits=200):\n",
    "                          \n",
    "    scoresA = []\n",
    "    scoresP = []\n",
    "    scoresR = []\n",
    "    \n",
    "    for n in range(0,n_splits):\n",
    "        inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val,random.randint(0,1000))\n",
    "        # scaling data according to training inputs\n",
    "        scaler_S = StandardScaler().fit(inp_tr)\n",
    "        inp_tr = scaler_S.transform(inp_tr)\n",
    "        inp_va = scaler_S.transform(inp_va)\n",
    "        estimator.fit(inp_tr, tar_tr.ravel())  \n",
    "        pred_va = estimator.predict(inp_va)\n",
    "        scoresA.append(accuracy_score(tar_va,pred_va))\n",
    "        scoresR.append(recall_score(tar_va,pred_va,average=None,zero_division=1))  \n",
    "        scoresP.append(precision_score(tar_va,pred_va,average=None,zero_division=1)) \n",
    "        print(f'n = {n}') \n",
    "    scoresR = list(map(list, zip(*scoresR)))\n",
    "    scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "    estimateA = np.mean(scoresA)*100.\n",
    "    stderrA = np.std(scoresA)*100.\n",
    "    \n",
    "    estimateR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.,np.mean(scoresR[3])*100.,np.mean(scoresR[4])*100.,np.mean(scoresR[5])*100.,np.mean(scoresR[6])*100.]\n",
    "    stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.,np.std(scoresR[3])*100.,np.std(scoresR[4])*100.,np.std(scoresR[5])*100.,np.std(scoresR[6])*100.]\n",
    "    \n",
    "    estimateP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.,np.mean(scoresP[3])*100.,np.mean(scoresP[4])*100.,np.mean(scoresP[5])*100.,np.mean(scoresP[6])*100.]\n",
    "    stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.,np.std(scoresP[3])*100.,np.std(scoresP[4])*100.,np.std(scoresP[5])*100.,np.std(scoresP[6])*100.]\n",
    "    \n",
    "    return estimateR, stderrR, estimateP, stderrP, estimateA, stderrA\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_estimate_MLP(NN, X, Y, n_splits=200, epochs =10000):\n",
    "                        \n",
    "    scoresA = []\n",
    "    scoresP = []\n",
    "    scoresR = []\n",
    "\n",
    "    for n in range(0,n_splits):\n",
    "        train_loader, val_loader = MLP_data_setup(X, Y)\n",
    "        val_predictions, val_truth_values = main(epochs,NN,optimizer,train_loader,val_loader)\n",
    "        scoresA.append(accuracy_score(val_truth_values,val_predictions))\n",
    "        scoresR.append(recall_score(val_truth_values,val_predictions,average=None,zero_division=1))  \n",
    "        scoresP.append(precision_score(val_truth_values,val_predictions,average=None,zero_division=1)) \n",
    "        print(f'n = {n}') \n",
    "    scoresR = list(map(list, zip(*scoresR)))\n",
    "    scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "    estimateA = np.mean(scoresA)*100.\n",
    "    stderrA = np.std(scoresA)*100.\n",
    "    \n",
    "    estimateR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.,np.mean(scoresR[3])*100.,np.mean(scoresR[4])*100.,np.mean(scoresR[5])*100.,np.mean(scoresR[6])*100.]\n",
    "    stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.,np.std(scoresR[3])*100.,np.std(scoresR[4])*100.,np.std(scoresR[5])*100.,np.std(scoresR[6])*100.]\n",
    "    \n",
    "    estimateP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.,np.mean(scoresP[3])*100.,np.mean(scoresP[4])*100.,np.mean(scoresP[5])*100.,np.mean(scoresP[6])*100.]\n",
    "    stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.,np.std(scoresP[3])*100.,np.std(scoresP[4])*100.,np.std(scoresP[5])*100.,np.std(scoresP[6])*100.]\n",
    "    \n",
    "    return estimateR, stderrR, estimateP, stderrP, estimateA, stderrA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(epochs, NetInstance, OptInstance, train_loader, val_loader, ScheduleInstance=None):\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss, train_predictions, train_truth_values = train(epoch, NetInstance, OptInstance, train_loader, device)\n",
    "        val_loss, val_predictions, val_truth_values = validate(NetInstance, val_loader, device)\n",
    "        \n",
    "        if ScheduleInstance is not None:\n",
    "            ScheduleInstance.step()\n",
    "\n",
    "    return val_predictions, val_truth_values\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_data_setup(X,Y):\n",
    "    inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val,random.randint(0,1000))\n",
    "            \n",
    "    # scaling data according to training inputs\n",
    "    scaler_S = StandardScaler().fit(inp_tr)\n",
    "    inp_tr = scaler_S.transform(inp_tr)\n",
    "    inp_va = scaler_S.transform(inp_va)\n",
    "    # inp_te = scaler_S.transform(inp_te) \n",
    "\n",
    "    # creation of tensor instances\n",
    "\n",
    "    inp_tr = torch.as_tensor(inp_tr)\n",
    "    tar_tr = torch.as_tensor(tar_tr)\n",
    "    inp_va = torch.as_tensor(inp_va)\n",
    "    tar_va = torch.as_tensor(tar_va)\n",
    "    # inp_te = torch.as_tensor(inp_te)\n",
    "    # tar_te = torch.as_tensor(tar_te)\n",
    "\n",
    "    # pass tensors into TensorDataset instances\n",
    "    train_data = data_utils.TensorDataset(inp_tr, tar_tr)\n",
    "    val_data = data_utils.TensorDataset(inp_va, tar_va)\n",
    "    # test_data = data_utils.TensorDataset(inp_te, tar_te)\n",
    "\n",
    "    # constructing data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=25, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=25, shuffle=True)\n",
    "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=25, shuffle=True)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nn instance\n",
    "TwoNN = TwoLayerMLP(8, 20, 7, weight_initialize=True)\n",
    "## load settings in\n",
    "optimizer = optim.SGD(TwoNN.parameters(), lr=4e-1, momentum=0.9)\n",
    "estR, stderrR, estP, stderrP, estA, stderrA  = bootstrap_estimate_MLP(TwoNN, X, Y, n_splits=200, epochs =3000)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Galaxies\", \"AGNs\", \"Shocks\", \"PAHs\", \"Stars\"]\n",
    "f = open(\"PRAScores_2LayerMLP_7Classes.txt\", \"w\")\n",
    "f.write(\"TwoLayerMLP Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "import random\n",
    "\n",
    "def bootstrap(NN,epochs):\n",
    "    train_loader, val_loader = MLP_data_setup(X, Y)\n",
    "    pred_va, tar_va = main(epochs,NN,optimizer,train_loader,val_loader)\n",
    "    ScoresA = accuracy_score(tar_va,pred_va)\n",
    "    ScoresR = recall_score(tar_va,pred_va,average=None,zero_division=1)\n",
    "    ScoresP = precision_score(tar_va,pred_va,average=None,zero_division=1)\n",
    "\n",
    "    return ScoresR, ScoresP, ScoresA\n",
    "\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\") # For original targets via Gutermuth 2009 Method\n",
    "\n",
    "amounts_train = [331,331,331,331,27,70,331]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "\n",
    "\n",
    "\n",
    "BaseNN = BaseMLP(8, 20, 7, weight_initialize=True)\n",
    "## load settings in\n",
    "optimizer = optim.SGD(BaseNN.parameters(), lr=4e-1, momentum=0.9)\n",
    "# estR, stderrR, estP, stderrP, estA, stderrA  = bootstrap_estimate_MLP(BaseNN, X, Y, n_splits=200, epochs =50000)\n",
    "iters = [(BaseNN,50000)] * 50\n",
    "ans = []\n",
    "for n in [0,1,2,3]:\n",
    "\n",
    "    with mp.Pool(12) as pool:\n",
    "        ans.append(pool.starmap(bootstrap, iters))\n",
    "    np.save(f\"intermediatesave_onelayerMLP_{n}.npy\",ans)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoresR = list(map(list, zip(*ans)))[0]\n",
    "scoresP = list(map(list, zip(*ans)))[1]\n",
    "scoresA = list(map(list, zip(*ans)))[2]\n",
    "\n",
    "\n",
    "scoresR = list(map(list, zip(*scoresR)))\n",
    "scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "\n",
    "estA = np.mean(scoresA)*100.\n",
    "stderrA = np.std(scoresA)*100.\n",
    "\n",
    "estR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.,np.mean(scoresR[3])*100.,np.mean(scoresR[4])*100.,np.mean(scoresR[5])*100.,np.mean(scoresR[6])*100.]\n",
    "stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.,np.std(scoresR[3])*100.,np.std(scoresR[4])*100.,np.std(scoresR[5])*100.,np.std(scoresR[6])*100.]\n",
    "\n",
    "estP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.,np.mean(scoresP[3])*100.,np.mean(scoresP[4])*100.,np.mean(scoresP[5])*100.,np.mean(scoresP[6])*100.]\n",
    "stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.,np.std(scoresP[3])*100.,np.std(scoresP[4])*100.,np.std(scoresP[5])*100.,np.std(scoresP[6])*100.]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 0\n",
      "n = 1\n",
      "n = 2\n",
      "n = 3\n",
      "n = 4\n",
      "n = 5\n",
      "n = 6\n",
      "n = 7\n",
      "n = 8\n",
      "n = 9\n",
      "n = 10\n",
      "n = 11\n",
      "n = 12\n",
      "n = 13\n",
      "n = 14\n",
      "n = 15\n",
      "n = 16\n",
      "n = 17\n",
      "n = 18\n",
      "n = 19\n",
      "n = 20\n",
      "n = 21\n",
      "n = 22\n",
      "n = 23\n",
      "n = 24\n",
      "n = 25\n",
      "n = 26\n",
      "n = 27\n",
      "n = 28\n",
      "n = 29\n",
      "n = 30\n",
      "n = 31\n",
      "n = 32\n",
      "n = 33\n",
      "n = 34\n",
      "n = 35\n",
      "n = 36\n",
      "n = 37\n",
      "n = 38\n",
      "n = 39\n",
      "n = 40\n",
      "n = 41\n",
      "n = 42\n",
      "n = 43\n",
      "n = 44\n",
      "n = 45\n",
      "n = 46\n",
      "n = 47\n",
      "n = 48\n",
      "n = 49\n",
      "n = 50\n",
      "n = 51\n",
      "n = 52\n",
      "n = 53\n",
      "n = 54\n",
      "n = 55\n",
      "n = 56\n",
      "n = 57\n",
      "n = 58\n",
      "n = 59\n",
      "n = 60\n",
      "n = 61\n",
      "n = 62\n",
      "n = 63\n",
      "n = 64\n",
      "n = 65\n",
      "n = 66\n",
      "n = 67\n",
      "n = 68\n",
      "n = 69\n",
      "n = 70\n",
      "n = 71\n",
      "n = 72\n",
      "n = 73\n",
      "n = 74\n",
      "n = 75\n",
      "n = 76\n",
      "n = 77\n",
      "n = 78\n",
      "n = 79\n",
      "n = 80\n",
      "n = 81\n",
      "n = 82\n",
      "n = 83\n",
      "n = 84\n",
      "n = 85\n",
      "n = 86\n",
      "n = 87\n",
      "n = 88\n",
      "n = 89\n",
      "n = 90\n",
      "n = 91\n",
      "n = 92\n",
      "n = 93\n",
      "n = 94\n",
      "n = 95\n",
      "n = 96\n",
      "n = 97\n",
      "n = 98\n",
      "n = 99\n",
      "n = 100\n",
      "n = 101\n",
      "n = 102\n",
      "n = 103\n",
      "n = 104\n",
      "n = 105\n",
      "n = 106\n",
      "n = 107\n",
      "n = 108\n",
      "n = 109\n",
      "n = 110\n",
      "n = 111\n",
      "n = 112\n",
      "n = 113\n",
      "n = 114\n",
      "n = 115\n",
      "n = 116\n",
      "n = 117\n",
      "n = 118\n",
      "n = 119\n",
      "n = 120\n",
      "n = 121\n",
      "n = 122\n",
      "n = 123\n",
      "n = 124\n",
      "n = 125\n",
      "n = 126\n",
      "n = 127\n",
      "n = 128\n",
      "n = 129\n",
      "n = 130\n",
      "n = 131\n",
      "n = 132\n",
      "n = 133\n",
      "n = 134\n",
      "n = 135\n",
      "n = 136\n",
      "n = 137\n",
      "n = 138\n",
      "n = 139\n",
      "n = 140\n",
      "n = 141\n",
      "n = 142\n",
      "n = 143\n",
      "n = 144\n",
      "n = 145\n",
      "n = 146\n",
      "n = 147\n",
      "n = 148\n",
      "n = 149\n",
      "n = 150\n",
      "n = 151\n",
      "n = 152\n",
      "n = 153\n",
      "n = 154\n"
     ]
    }
   ],
   "source": [
    "# # create nn instance\n",
    "# BaseNN = BaseMLP(8, 20, 7, weight_initialize=True)\n",
    "# ## load settings in\n",
    "# optimizer = optim.SGD(BaseNN.parameters(), lr=4e-1, momentum=0.9)\n",
    "# estR, stderrR, estP, stderrP, estA, stderrA  = bootstrap_estimate_MLP(BaseNN, X, Y, n_splits=200, epochs =50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Galaxies\", \"AGNs\", \"Shocks\", \"PAHs\", \"Stars\"]\n",
    "f = open(\"PRAScores_1LayerMLP_7Classes.txt\", \"w\")\n",
    "f.write(\"OneLayerMLP & Recall & Precision & Accuracy//\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7a87715bbc43d8b5f73b6200b6ef66f163e7bfd9f5c97aea1eada326c99da2f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
