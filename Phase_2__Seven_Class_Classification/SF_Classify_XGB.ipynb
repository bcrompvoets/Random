{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcrompvoets/Star_Formation/blob/main/SF_Classify_XGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrNFkZNWhbUB"
   },
   "source": [
    "# Classifying Class I, II, and others using XGBoost\n",
    "\n",
    "Using data from the four IRAC bands (3.6, 4.5, 5.8, and 8  Î¼m), we classify each object as \"other\", Class I or Class II protostars. We use a XGBoost Classifier with default values. \n",
    "\n",
    "This data comes from Cornu and Montillaud (2021) (https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/647/A116) and includes Spitzer data of the Orion and NGC 2264 star forming regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1NjBTq--6x14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breannacrompvoets/opt/anaconda3/envs/SF/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classic ML libraries\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_estimate_and_ci(estimator, X_tr, y_tr, X_va, y_va, scoring_func=None, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200):\n",
    "                        \n",
    "    scores = []\n",
    "\n",
    "    if scoring_func == accuracy_score:\n",
    "        for n in range(0,n_splits):\n",
    "            estimator.fit(X_tr, y_tr.ravel())  \n",
    "            scores.append(scoring_func(y_va,estimator.predict(X_va)))\n",
    "            # scores = list(map(list, zip(*scores)))\n",
    "        estimate = np.mean(scores)*100.\n",
    "        stderr = np.std(scores)*100. \n",
    "\n",
    "    else:\n",
    "        for n in range(0,n_splits):\n",
    "            estimator.fit(X_tr, y_tr.ravel())  \n",
    "            scores.append(scoring_func(y_va,estimator.predict(X_va),average=None))   \n",
    "            scores = list(map(list, zip(*scores)))\n",
    "    \n",
    "        estimate = [np.mean(scores[0])*100.,np.mean(scores[1])*100.,np.mean(scores[2])*100.,np.mean(scores[3])*100.,np.mean(scores[4])*100.,np.mean(scores[5])*100.,np.mean(scores[6])*100.]\n",
    "        stderr = [np.std(scores[0])*100.,np.std(scores[1])*100.,np.std(scores[2])*100.,np.std(scores[3])*100.,np.std(scores[4])*100.,np.std(scores[5])*100.,np.std(scores[6])*100.]\n",
    "    \n",
    "    return estimate, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fGsfJbnnL_A",
    "outputId": "56d29dd7-b00f-4fee-e523-00f01ebc08c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of Datasets : Inputs , Targets\n",
      "------------------------------------\n",
      "Training set: (1752, 8) , (1752,) \n",
      "Validation set: (5377, 8) , (5377,) \n",
      "Testing Set: (19774, 8), (19774,)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\") # For original targets via Gutermuth 2009 Method\n",
    "\n",
    "\n",
    "# custom data loader to pull in custom sized data set\n",
    "# use seed to get replicable results for now\n",
    "seed_val = 1111\n",
    "\n",
    "# the amounts below are how many of each class of object you want in the training set and validation set - leftover amounts given to testing set\n",
    "\n",
    "# CM21 Split\n",
    "# amounts_train = [331,1141,231,529,27,70,1257]\n",
    "amounts_train = [331,331,331,331,27,70,331]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "\n",
    "# calling custom datagrabber here\n",
    "inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val, seed_val)\n",
    "\n",
    "# scaling data according to training inputs\n",
    "scaler_S = StandardScaler().fit(inp_tr)\n",
    "inp_tr = scaler_S.transform(inp_tr)\n",
    "inp_va = scaler_S.transform(inp_va)\n",
    "inp_te = scaler_S.transform(inp_te) # Comment out for 75/25 split\n",
    "\n",
    "# printouts for double checking all the sets and amounts\n",
    "print('Sizes of Datasets : Inputs , Targets')\n",
    "print('------------------------------------')\n",
    "print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c71_3Uj_8hhy"
   },
   "outputs": [],
   "source": [
    "# xgb_1 = xgb.XGBClassifier(use_label_encoder=False,eval_metric='mlogloss',gamma=1)\n",
    "\n",
    "# parameters = {'subsample':[0.5,1.0],'max_depth':np.arange(1,11,2),'sampling_method':['uniform']}\n",
    "# xgbcl = GridSearchCV(xgb_1, parameters)\n",
    "\n",
    "\n",
    "xgbcl = xgb.XGBClassifier(max_depth=7,sampling_method='uniform',subsample=1.0,gamma=2,use_label_encoder=False,eval_metric='mlogloss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5E_6Xtu2a-_i",
    "outputId": "0d448651-e394-4e0b-d175-da2636756294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 s, sys: 3.4 s, total: 9.92 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgbcl.fit(inp_tr,tar_tr.ravel())  # fit the model with training set\n",
    "# save in JSON format\n",
    "xgbcl.save_model(\"XGB_Settings.json\")\n",
    "# save in text format\n",
    "xgbcl.save_model(\"XGB_Settings.txt\")\n",
    "\n",
    "# Find the predicted values\n",
    "# pred_tr = xgbcl.predict(inp_tr)\n",
    "# pred_va = xgbcl.predict(inp_va)\n",
    "\n",
    "# print(classification_report(tar_va,pred_va))\n",
    "# print(classification_report(tar_tr,pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_tr = []\n",
    "# f1_va = []\n",
    "# G = np.arange(0,15)\n",
    "# for g in G:\n",
    "#     xgbcl = xgb.XGBClassifier(gamma=g,max_depth=7,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "#     xgbcl.fit(inp_tr,tar_tr)\n",
    "#     pred_tr = xgbcl.predict(inp_tr)\n",
    "#     pred_va = xgbcl.predict(inp_va)\n",
    "\n",
    "#     f1_tr.append(f1_score(tar_tr,pred_tr,average=None))\n",
    "#     f1_va.append(f1_score(tar_va,pred_va,average=None))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(1,1,figsize=(12,8))\n",
    "# import matplotlib.pylab as pl\n",
    "\n",
    "# n = 7\n",
    "# colors = pl.cm.Reds(np.linspace(0,1,7))\n",
    "\n",
    "# # plt.set_cmap('Blues')\n",
    "# plt.plot(G,f1_tr,label=['T C1','T_C2','T_Ga','T_A','T_Sh','T_P','T_St'])\n",
    "\n",
    "# # plt.set_cmap('Reds')\n",
    "# plt.plot(G,f1_va,label='Validate')\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xgbcl.best_params_)\n",
    "# print(xgbcl.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LI79NmRwh9nj",
    "outputId": "84a7180b-350f-4fdc-ef18-c3cae3c7c3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 38s, sys: 4min 12s, total: 22min 51s\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Galaxies\", \"AGNs\", \"Shocks\", \"PAHs\", \"Stars\"]\n",
    "f = open(\"PRAScores_XGB_7Classes.txt\", \"w\")\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.3f}\".format(estR[i])+\"\\pm\"+\"{:.3f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.3f}\".format(estP[i])+\"\\pm\"+\"{:.3f}\".format(stderrP[i])+\"$ & $\"+\"{:.3f}\".format(estA)+\"\\pm\"+\"{:.3f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.3f}\".format(estR[i])+\"\\pm\"+\"{:.3f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.3f}\".format(estP[i])+\"\\pm\"+\"{:.3f}\".format(stderrP[i])+\"$&// \\n\")\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SF-Classify-XGB",
   "provenance": []
  },
  "interpreter": {
   "hash": "b32a61eb6ff93be71f4251ffff73c84aa9d3b2a4cb02800e928ae707ac05d999"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('SF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
