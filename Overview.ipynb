{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GLopVOsCQ2A",
    "tags": []
   },
   "source": [
    "# Project on testing ML techniques to identify YSOs in Spitzer IRAC data\n",
    "\n",
    "## Breanna Crompvoets and Samuel Fielder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r3MvlnrCQ2B",
    "tags": []
   },
   "source": [
    "## Project Summary and Goals\n",
    "Young Stellar Objects (YSOs) are newly forming stars which are yet to begin burning. They are split into different classes depending on their dust/gas envelope to protostar ratio; Class 0 having the greatest envelope and Class III no longer having an envelope. Due to the different ratios of envelope to protostar, each class appears differently in spectroscopy; thus the difference in fluxes between Spitzer IRAC bands are able to determine which class the data comes from. This project will focus on using the same data as Cornu and Montillaud (2021; CM21) to classify data points into three classes: Class I, Class II, and Contaminants. Only these three classes out of the original 9 available are chosen as Class 0 is too dusty to detect, and Class III is difficult to distinguish from regular stars. Furthermore, the contaminating classes (galaxies, shocks, stars, and PAHs) are of less concern -- we would like the algorithms to focus on distinguishing Class I and Class II from the rest. The original paper uses a multi-layer perceptron (MLP) with one hidden layer (20 neurons). Their results are presented in the below table.\n",
    "\n",
    "|Class | Recall | Precision |\n",
    "| --- | --- | --- |\n",
    "|1 | 94.0% | 79.1 %|\n",
    "|2 | 96.7% | 90.6% | \n",
    "|Other | 98.7%| 99.8%| \n",
    "\n",
    "The data for this project was pulled from https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/647/A116. These data include columns for four Spitzer IRAC bands (3.6 $\\mu m$, 4.5 $\\mu m$, 5.8 $\\mu m$, and 8 $\\mu m$) fluxes and errors, as well as from one Spitzer MIPS band (24 $\\mu m$), along with the target values as determined via a manual classification scheme and the predicted data from CM21. We will only be using the four IRAC bands and their associated errors, as the MIPS band does not provide data for most objects. We use the same target values as they do for accurate comparison. \n",
    "\n",
    "This project seeks to use a multitude of algorithms learned over the semester to measure their effectiveness and compare it to the recreated MLP of CM21. These algorithms include: GridSearch with an SVC, GridSearch with a Logistic Regressor, a Stacking Ensemble with an SVC and a Logistic Regressor, a Gradient Boosting ensemble, a Random Forest ensemble, and an XGBoost ensemble. We also created our own MLP based off of their prescription. The workload was split as follows: B. Crompvoets completed the data cleaning and all other algorithms besides the MLP, and S. Fielder completed an MLP close to that of CM21, as well as creating a custom data loader/split. They communicated together on the best hyper-parameters to test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq3kOX1MH-wS"
   },
   "source": [
    "## Import Libraries and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oNZdspOvCQ2C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breannacrompvoets/opt/anaconda3/envs/SF/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classic ML libraries\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hpioJ6S3CQ2E"
   },
   "outputs": [],
   "source": [
    "# settings for confusion matrix plots and classification reports\n",
    "cm_blues = plt.cm.Blues\n",
    "custom_labs = ['Class 1', 'Class 2', 'Others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RJ3awIPCQ2D"
   },
   "source": [
    "# Classical ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8QmwVqNCQ2F"
   },
   "source": [
    "For each of the below algorithms we ran a gridsearch over a wide variety of hyperparameters. These parameter dictionaries are commented out in each cell, and we use the best parameters as a sample here to show how each algorithm performed. This section of the project was conducted by B. Crompvoets.\n",
    "\n",
    "We conducted these fits for each of three data splits:\n",
    "* \"75/25\" -- here the data is split into 75% training, 25% test set.\n",
    "* \"300s\" -- here the data is split such that 5 out of the 7 subclasses, each has 300 members in the training set (2 do not have enough members). The test set size is the same as CM21. The data is again split into only 3 classes to train/test.\n",
    "* \"CM21\" -- here the data is split with the exact same values as CM21 provide in their paper.\n",
    "\n",
    "An example of a run with the 75/25 split is given below, with best results for each of the other runs commented out, as well as the parameter grid tested for each GridSearch. The results of all runs will be included at the end of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-unH4thCQ2F"
   },
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uaJUpxFCQ2G",
    "outputId": "e9df8de9-9b57-477a-d9c0-9fe62741f2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of Datasets : Inputs , Targets\n",
      "------------------------------------\n",
      "Training set: (1597, 8) , (1597,) \n",
      "Validation set: (5377, 8) , (5377,) \n",
      "Testing Set: (19929, 8), (19929,)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\") # For original targets via Gutermuth 2009 Method\n",
    "# Y = np.load(\"Pred_Class_AllClasses_Sep.npy\") # For predicted targets from CM21\n",
    "\n",
    "\n",
    "# custom data loader to pull in custom sized data set\n",
    "# use seed to get replicable results for now\n",
    "seed_val = 1111\n",
    "\n",
    "# the amounts below are how many of each class of object you want in the training set and validation set - leftover amounts given to testing set\n",
    "\n",
    "# CM21 Split\n",
    "# amounts_train = [331,1141,231,529,27,70,1257]\n",
    "# amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [331,1141,231+529+27+70+1257]\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "\n",
    "# 300s Split\n",
    "amounts_train = [300,300,300,300,27,70,300]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [300,300,300+300+27+70+300]\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "\n",
    "# 75/25 Split\n",
    "# amounts_train = [311,1994,391,1043,25,66,21796] #75/25 train\n",
    "# amounts_val = [103,665,130,348,9,22,5449] #75/25 val\n",
    "# amounts_train = [311,1994,391+1043+25+66+21796] #75/25 train\n",
    "# amounts_val = [103,665,130+348+9+22+5449] #75/25 val\n",
    "\n",
    "# calling custom datagrabber here\n",
    "inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val, seed_val)\n",
    "\n",
    "# scaling data according to training inputs\n",
    "scaler_S = StandardScaler().fit(inp_tr)\n",
    "inp_tr = scaler_S.transform(inp_tr)\n",
    "inp_va = scaler_S.transform(inp_va)\n",
    "\n",
    "# COMMENT NEXT LINE IF RUNNING 75/25 SPLIT\n",
    "inp_te = scaler_S.transform(inp_te) # Comment out for 75/25 split\n",
    "\n",
    "# printouts for double checking all the sets and amounts\n",
    "print('Sizes of Datasets : Inputs , Targets')\n",
    "print('------------------------------------')\n",
    "print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((inp_tr,inp_va,inp_te))\n",
    "targets = np.concatenate((tar_tr,tar_va,tar_te))\n",
    "np.save(\"XGB_Val_G-targets7.npy\",targets)\n",
    "tar_tr = np.where(tar_tr<2,tar_tr,2)\n",
    "tar_va = np.where(tar_va<2,tar_va,2)\n",
    "tar_te = np.where(tar_te<2,tar_te,2)\n",
    "targets = np.concatenate((tar_tr,tar_va,tar_te))\n",
    "np.save(\"XGB_Val_G-targets2.npy\",targets)\n",
    "\n",
    "xgbcl = xgb.XGBClassifier(max_depth=7,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "xgbcl.fit(inp_tr,tar_tr)\n",
    "pred_va = xgbcl.predict(inputs)\n",
    "np.save(\"XGB_300s_G-targets_VPred.npy\",pred_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_estimate_and_ci(estimator, X_tr, y_tr, X_va, y_va, scoring_func=None, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200):\n",
    "                        \n",
    "    scores = []\n",
    "\n",
    "    if scoring_func == accuracy_score:\n",
    "        for n in range(0,n_splits):\n",
    "            estimator.fit(X_tr, y_tr.ravel())  \n",
    "            scores.append(scoring_func(y_va,estimator.predict(X_va)))\n",
    "            # scores = list(map(list, zip(*scores)))\n",
    "        estimate = np.mean(scores)\n",
    "        lower_bound = np.percentile(scores, 100*(alpha/2))\n",
    "        upper_bound = np.percentile(scores, 100*(1-alpha/2))\n",
    "        stderr = np.std(scores)   \n",
    "\n",
    "    else:\n",
    "        for n in range(0,n_splits):\n",
    "            estimator.fit(X_tr, y_tr.ravel())  \n",
    "            scores.append(scoring_func(y_va,estimator.predict(X_va),average=None))   \n",
    "            scores = list(map(list, zip(*scores)))\n",
    "    \n",
    "        estimate = [np.mean(scores[0]),np.mean(scores[1]),np.mean(scores[2])]\n",
    "        lower_bound = [np.percentile(scores[0], 100*(alpha/2)),np.percentile(scores[1], 100*(alpha/2)),np.percentile(scores[2], 100*(alpha/2))]\n",
    "        upper_bound = [np.percentile(scores[0], 100*(1-alpha/2)),np.percentile(scores[1], 100*(1-alpha/2)),np.percentile(scores[2], 100*(1-alpha/2))]\n",
    "        stderr = [np.std(scores[0]),np.std(scores[1]),np.std(scores[2])]\n",
    "    \n",
    "    return estimate, lower_bound, upper_bound, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"PRAScores_7525_C-targets_2.txt\",\"w\")\n",
    "f.write(\"C-targets \\n\")\n",
    "f.write(\"7525 split, correct \\n\")\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM4fxaVCQ2I"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "\n",
    "Specifying logistic regression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "hyperparameters tested over initially\n",
    "param_grid = [{'penalty': ['l1'], 'max_iter': np.arange(300,1500,100),\n",
    "        'solver': ['liblinear', 'saga'], 'tol': np.arange(0.0001,0.01,0.0005)},\n",
    "        {'penalty': ['l2'], 'max_iter': np.arange(300,1500,100),\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'tol': np.arange(0.0001,0.01,0.0005)},\n",
    "        {'penalty': ['elasticnet'], 'max_iter': np.arange(100,2000,100), 'l1_ratio': np.arange(0.1,1.,0.1),\n",
    "        'solver': ['saga'], 'tol': np.arange(0.0001,0.01,0.0005)}]\n",
    "\n",
    "grid = GridSearchCV(logreg,param_grid=param_grid, verbose=1)\n",
    "\n",
    "Run the data through the grid to find optimal results\n",
    "grid.fit(inp_tr, tar_tr.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lr2hNedVCQ2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 12.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 75/25\n",
    "logreg = LogisticRegression('l1',max_iter=500,solver='saga',tol=0.0001)\n",
    "# 300s\n",
    "# logreg = LogisticRegression('l1',max_iter=300,solver='saga',tol=0.0016)\n",
    "# CM21\n",
    "# logreg = LogisticRegression('l1',max_iter=600,solver='saga',tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(logreg, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=recall_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                              \n",
    "f.write(\"LR Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(logreg, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"LR Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(logreg, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"LR Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56H3DN1ICQ2K"
   },
   "source": [
    "## SVM\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'kernel':['rbf','sigmoid','linear','poly'], 'gamma':['auto','scale'], 'C':np.arange(0.1,1.,0.1)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1MOXdkACQ2K",
    "outputId": "d055072c-5552-4a57-e7ac-cf340a67e0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 µs, sys: 151 µs, total: 274 µs\n",
      "Wall time: 66.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final hyperparameters\n",
    "# 75/25\n",
    "svc = SVC(kernel='rbf',gamma='auto',C=0.9)\n",
    "# 300s\n",
    "# svc = SVC(kernel='linear',gamma='auto',C=0.8)\n",
    "# CM21\n",
    "# svc = SVC(kernel='rbf',gamma='auto',C=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onNpzooWWLlX",
    "outputId": "1f817af8-35c4-4d65-d019-e2c97dfb9146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(svc, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"SVC Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(svc, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"SVC Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(svc, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"SVC Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z4DZPwFTZEP"
   },
   "source": [
    "## SVM/LR Stacking Ensemble\n",
    "No GridSearch, uses best pars as defined previously. Adding multiple SVC's does not improve results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "wd6LgbdVTduX",
    "outputId": "8013ddeb-ef74-41c4-9494-13282be076ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 33 µs, total: 61 µs\n",
      "Wall time: 62 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Specify Gradient Boost\n",
    "# 75/25\n",
    "estimators = [('svc', SVC(kernel='rbf',gamma='auto',C=0.9,random_state=42))]\n",
    "# 300s\n",
    "# estimators = [('svc', SVC(C=0.8, gamma='auto', kernel='linear',random_state=42))]\n",
    "# CM21 \n",
    "# estimators = [('svc', SVC(C=0.9, gamma='auto', kernel='rbf',random_state=42))]\n",
    "\n",
    "# As the parameters for the Logistic Regression didn't change much, we use the best pars from the first trial.\n",
    "stacl = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(penalty = 'l1', max_iter = 500, solver ='saga', tol =0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ynA7OC3hUlHg",
    "outputId": "fd2b0ccb-429b-46d2-e85b-5ad69ca93359"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(stacl, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"Stack Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(stacl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"Stack Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(stacl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"Stack Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzcB_s2NCQ2L"
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'n_estimators': np.arange(50,250,50),'subsample':[0.5,1.0],\n",
    "              'criterion':['friedman_mse'],'n_iter_no_change':[5],'warm_start':[True,False],\n",
    "              'max_depth':np.arange(1,11,2),'max_features': ['sqrt','log2']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "L6GloAZcCQ2M",
    "outputId": "bbe6b011-4640-4876-906f-e232a1fb5754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 µs, sys: 52 µs, total: 77 µs\n",
      "Wall time: 81.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final hyperparameters\n",
    "# 75/25\n",
    "boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=9,max_features='log2',\n",
    "                n_estimators=50,n_iter_no_change=5,subsample=1.0,warm_start=True)\n",
    "\n",
    "# 300s\n",
    "# boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=5,max_features='log2',\n",
    "#                 n_estimators=150,n_iter_no_change=5,subsample=1.0,warm_start=False)\n",
    "\n",
    "# CM21\n",
    "# boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=7,max_features='log2',\n",
    "#                 n_estimators=200,n_iter_no_change=5,subsample=1.0,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GId2kQ7_CQ2M",
    "outputId": "efb6aba3-eb49-4e4b-b0b0-a60629ddd6df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(boostcl, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"GB Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(boostcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"GB Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(boostcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"GB Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v379VtHtCQ2N"
   },
   "source": [
    "### XGBoost\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'subsample':[0.5,1.0],'max_depth':np.arange(1,11,2),'sampling_method':['uniform']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI4UnZIuCQ2N",
    "outputId": "d2456ea3-a4e4-4b4e-beb7-a4565c0de316"
   },
   "outputs": [],
   "source": [
    "# 75/25\n",
    "xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "# 300s\n",
    "# xgbcl = xgb.XGBClassifier(max_depth=7,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "# CM21\n",
    "# xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=1.0,use_label_encoder=False,eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjT1jfOuS-c7",
    "outputId": "1c76e70a-d7c7-4e67-e646-7adecbc107b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"XGB Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"XGB Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(xgbcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"XGB Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6A0mcoACQ2N"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'class_weight': ['balanced_subsample','balanced'], 'n_estimators': np.arange(50,250,50),\n",
    "        'criterion': ['gini', 'entropy'], 'max_features': ['sqrt','log2'], 'oob_score':[True,False]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n4qZO8L_CQ2O"
   },
   "outputs": [],
   "source": [
    "# Final hyperparameters\n",
    "# 75/25\n",
    "rfcl = RandomForestClassifier(class_weight='balanced_subsample',criterion='entropy',max_features='log2',n_estimators=150,oob_score=False)\n",
    "# 300s\n",
    "# rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=50,oob_score=False)\n",
    "# CM21\n",
    "# rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=100,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KHU1_UQ2CQ2O",
    "outputId": "a1a4039b-c207-41ea-cc8c-b6e2b2d09feb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est, low, up, stderr = bootstrap_estimate_and_ci(rfcl, inp_tr, tar_tr.ravel(),  inp_va, tar_va, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"RF Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(rfcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"RF Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\"$\\pm$\"+\"{:.3f}\".format(stderr[0])+\"_{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\"$\\pm$\"+\"{:.3f}\".format(stderr[1])+\"_{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"}//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\"$\\pm$\"+\"{:.3f}\".format(stderr[2])+\"_{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"}//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci(rfcl, inp_tr, tar_tr.ravel(), inp_va, tar_va.ravel(), scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"RF Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\"$\\pm$\"+\"{:.3f}\".format(stderr)+\"_{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"}//\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hxFSJaxIhcvZ",
    "outputId": "a34455c2-ef18-4c6e-f3c8-00821f87bdf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system('say \"tadaaa your program has probably failed\"')\n",
    "os.system('say \"beep\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLJGW1U6CQ2O",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qp5HGBdhcvZ"
   },
   "source": [
    "In this section, we describe in which ways we replicated the CM21 paper, and the choices we made along the way to derive the best results we were able to achieve. This section was performed by S. Fielder.\n",
    "\n",
    "All the training has been performed prior to this notebook, and the state of the network saved at the appropriate time. Below we will just import the state of the system described, and run the testing set through for evaluation, and metrics along with Confusion Matrixes will be outputted for each subsection.\n",
    "\n",
    "Many of the functions called below are semi-custom made, and are found in the appropriate `.py` files in this directory. The majority come from `NN_Defs.py` where the model construction along with the training and validation functions are located. Additionally, our custom data split maker is found in `custom_dataloader.py`, in which we can build reproducable sets of training, validation and testing sets depending on the number of subclasses wanted in each set. In this example, we focus on the results achieved by using the CM21 data-split as performed using this loader.\n",
    "\n",
    "Finally `network_runner.py` was the script used in order to train all of the networks, and appropriately print out the metrics, along with saving plots for both confusion matrixes and loss values. Some of these outputs are shown in the `Saved_Final_Data/` directory. The settings found therein will be imported below to load the state of the networks as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdxoY6vtCQ2P"
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nfkaxzihCQ2P",
    "outputId": "d861bc81-c67d-4342-fec0-282577a69705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on : cpu\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "\n",
    "# custom script inputs\n",
    "from NN_Defs import get_n_params, train, validate, BaseMLP\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on : {device}')\n",
    "\n",
    "datadir = 'Saved_Final_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWSjzREdhcva"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1AKc4LsLhcva",
    "outputId": "9edb5e69-3f92-414c-c400-0c23cde99fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of Datasets : Inputs , Targets\n",
      "------------------------------------\n",
      "Training set: (3586, 8) , (3586,) \n",
      "Validation set: (5377, 8) , (5377,) \n",
      "Testing Set: (17940, 8), (17940,)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data\n",
    "# data load\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\")\n",
    "# Y = np.load(\"Pred_Class_AllClasses_Sep.npy\") # For predicted targets from CM21\n",
    "\n",
    "# custom data loader to pull in custom sized data set\n",
    "# use seed to get replicable results for now\n",
    "seed_val = 1111\n",
    "\n",
    "# the amounts below are how many of each class of object you want in the training set and validation set - leftover amounts given to testing set\n",
    "\n",
    "\n",
    "# CM21 Split\n",
    "amounts_train = [331,1141,231,529,27,70,1257]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [331,1141,231+529+27+70+1257] # C-targets\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "# 300s Split\n",
    "# amounts_train = [300,300,300,300,27,70,300]\n",
    "# amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [300,300,300+300+27+70+300]\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "# # 75/25 Split\n",
    "# amounts_train = [311,1994,391,1043,25,66,21796] #75/25 train\n",
    "# amounts_val = [103,665,130,348,9,22,5449] #75/25 val\n",
    "# amounts_train = [311,1994,391+1043+25+66+21796] #75/25 train\n",
    "# amounts_val = [103,665,130+348+9+22+5449] #75/25 val\n",
    "\n",
    "\n",
    "# calling custom datagrabber here\n",
    "inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val, seed_val)\n",
    "\n",
    "# scaling data according to training inputs\n",
    "scaler_S = StandardScaler().fit(inp_tr)\n",
    "inp_tr = scaler_S.transform(inp_tr)\n",
    "inp_va = scaler_S.transform(inp_va)\n",
    "# inp_te = scaler_S.transform(inp_te) # Comment out for 75/25 split\n",
    "\n",
    "# printouts for double checking all the sets and amounts\n",
    "print('Sizes of Datasets : Inputs , Targets')\n",
    "print('------------------------------------')\n",
    "print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXQgSJnOCQ2P"
   },
   "source": [
    "## Scaling, Conversions to Tensors, and DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.concatenate((inp_tr,inp_va,inp_te))\n",
    "targets = np.concatenate((tar_tr,tar_va,tar_te))\n",
    "np.save(\"MLP_Val_G-targets7.npy\",targets)\n",
    "tar_tr = np.where(tar_tr<2,tar_tr,2)\n",
    "tar_va = np.where(tar_va<2,tar_va,2)\n",
    "tar_te = np.where(tar_te<2,tar_te,2)\n",
    "inputs1 = np.concatenate((inp_tr,inp_va,inp_te))\n",
    "targets1 = np.concatenate((tar_tr,tar_va,tar_te))\n",
    "inputs1 = torch.tensor(inputs1)\n",
    "targets1 = torch.tensor(targets1)\n",
    "all_data = data_utils.TensorDataset(inputs1, targets1)\n",
    "all_loader = torch.utils.data.DataLoader(all_data, batch_size=25, shuffle=True)\n",
    "np.save(\"MLP_Val_G-targets2.npy\",targets1)\n",
    "loadpath = datadir+'Final_CSplit_4e-2_Settings'\n",
    "BaseNN = BaseMLP(8, 20, 3)\n",
    "BaseNN.load_state_dict(torch.load(loadpath, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy, val_predictions, val_truth_values = validate(BaseNN, all_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"MLP_CM21_G-targets_VPred.npy\",val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.02      0.02       414\n",
      "           1       0.10      0.07      0.08      2659\n",
      "           2       0.89      0.92      0.90     23830\n",
      "\n",
      "    accuracy                           0.82     26903\n",
      "   macro avg       0.33      0.33      0.33     26903\n",
      "weighted avg       0.79      0.82      0.81     26903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets1,val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y3xYPWWpCQ2Q"
   },
   "outputs": [],
   "source": [
    "# creation of tensor instances\n",
    "\n",
    "inp_tr = torch.tensor(inp_tr)\n",
    "tar_tr = torch.tensor(tar_tr)\n",
    "inp_va = torch.tensor(inp_va)\n",
    "tar_va = torch.tensor(tar_va)\n",
    "# inp_te = torch.tensor(inp_te)\n",
    "# tar_te = torch.tensor(tar_te)\n",
    "\n",
    "# pass tensors into TensorDataset instances\n",
    "train_data = data_utils.TensorDataset(inp_tr, tar_tr)\n",
    "val_data = data_utils.TensorDataset(inp_va, tar_va)\n",
    "# test_data = data_utils.TensorDataset(inp_te, tar_te)\n",
    "\n",
    "# constructing data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=25, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=25, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_estimate_and_ci_MLP(NN, valid_loader, device,  scoring_func=None, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200):\n",
    "                        \n",
    "    scores = []\n",
    "\n",
    "    if scoring_func == accuracy_score:\n",
    "        for n in range(0,n_splits):\n",
    "            val_loss, val_accuracy, val_predictions, val_truth_values = validate(NN, valid_loader, device)\n",
    "            scores.append(scoring_func(val_truth_values,val_predictions))\n",
    "        estimate = np.mean(scores)\n",
    "        lower_bound = np.percentile(scores, 100*(alpha/2))\n",
    "        upper_bound = np.percentile(scores, 100*(1-alpha/2))\n",
    "        stderr = np.std(scores)\n",
    "\n",
    "    else:\n",
    "        for n in range(0,n_splits):\n",
    "            val_loss, val_accuracy, val_predictions, val_truth_values = validate(BaseNN, val_loader, device)\n",
    "            scores.append(scoring_func(val_truth_values,val_predictions,average=None))   \n",
    "            scores = list(map(list, zip(*scores)))\n",
    "    \n",
    "        estimate = [np.mean(scores[0]),np.mean(scores[1]),np.mean(scores[2])]\n",
    "        lower_bound = [np.percentile(scores[0], 100*(alpha/2)),np.percentile(scores[1], 100*(alpha/2)),np.percentile(scores[2], 100*(alpha/2))]\n",
    "        upper_bound = [np.percentile(scores[0], 100*(1-alpha/2)),np.percentile(scores[1], 100*(1-alpha/2)),np.percentile(scores[2], 100*(1-alpha/2))]\n",
    "        stderr = [np.std(scores[0]),np.std(scores[1]),np.std(scores[2])]\n",
    "    \n",
    "    return estimate, lower_bound, upper_bound, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtqRXkQgCQ2Q"
   },
   "source": [
    "## Create Network Instance and Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "PkuLOdYaCQ2S"
   },
   "outputs": [],
   "source": [
    "# create nn instance\n",
    "BaseNN = BaseMLP(8, 20, 3)\n",
    "# load in saved state of network\n",
    "# loadpath = datadir+'Final_CSplit_4e-2_Settings' #CM21 Split\n",
    "loadpath = datadir+'Final_CSplit_CM21Pred_Settings' #CM21 Split - C-tar train\n",
    "# loadpath = datadir+'Final_300sSplit_CM21Pred_Settings' #300 Split - C-tar train\n",
    "# loadpath = datadir+'Final_300s_Settings'\n",
    "# loadpath = datadir+'Final_7525Split_CM21Pred_Settings' # 75/25 Split - Ctar train\n",
    "# loadpath = datadir+'Final_7525Split_Settings' # 75/25 Split\n",
    "\n",
    "BaseNN.load_state_dict(torch.load(loadpath, map_location=device))\n",
    "\n",
    "# compute validation results\n",
    "# val_loss, val_accuracy, val_predictions, val_truth_values = validate(BaseNN, val_loader, device)\n",
    "\n",
    "f = open(\"PRAScores_CM21_C-targets_MLP_2.txt\",\"w\")\n",
    "f.write(\"C-targets, CM21 data-split, correct\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"MLP Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\" _{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"} , \"+\"{:.3f}\".format(stderr[0])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\" _{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"} , \"+\"{:.3f}\".format(stderr[1])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\" _{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"} , \"+\"{:.3f}\".format(stderr[2])+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"MLP Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\" _{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"} , \"+\"{:.3f}\".format(stderr[0])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\" _{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"} , \"+\"{:.3f}\".format(stderr[1])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\" _{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"} , \"+\"{:.3f}\".format(stderr[2])+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"MLP Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\" _{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"} , \"+\"{:.3f}\".format(stderr)+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcD3yCvfhcvc"
   },
   "source": [
    "Again, the loss plot below was constructed at our last epoch in this specific model loaded. This is loaded in manually here from `/Saved_Final_Data`.\n",
    "\n",
    "<img src=\"Saved_Final_Data/Final_CSplit_4e-2_loss.png\" alt=\"Loss with 4e-5 Learning Rate\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b32a61eb6ff93be71f4251ffff73c84aa9d3b2a4cb02800e928ae707ac05d999"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
