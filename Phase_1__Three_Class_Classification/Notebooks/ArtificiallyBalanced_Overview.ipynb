{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GLopVOsCQ2A",
    "tags": []
   },
   "source": [
    "# Testing ML techniques to identify YSOs in Spitzer IRAC data\n",
    "\n",
    "## Breanna Crompvoets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq3kOX1MH-wS",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Libraries and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oNZdspOvCQ2C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breannacrompvoets/miniforge3/envs/SF/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classic ML libraries\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data\n",
    "custom_labs = ['Class 1', 'Class 2', 'Others']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RJ3awIPCQ2D",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Classical ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-unH4thCQ2F"
   },
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uaJUpxFCQ2G",
    "outputId": "e9df8de9-9b57-477a-d9c0-9fe62741f2d4"
   },
   "outputs": [],
   "source": [
    "# data load\n",
    "X = np.load(\"../inp.npy\")\n",
    "Y = np.load(\"../tar.npy\") \n",
    "\n",
    "# inp_tr, inp_va, tar_tr, tar_va = train_test_split(X,Y)\n",
    "\n",
    "# inp_te = np.load(\"../inp_test.npy\")\n",
    "# tar_te = np.load(\"../tar_test.npy\")\n",
    "\n",
    "\n",
    "# # scaling data according to training inputs\n",
    "# scaler_S = StandardScaler().fit(inp_tr)\n",
    "# inp_tr = scaler_S.transform(inp_tr)\n",
    "# inp_va = scaler_S.transform(inp_va)\n",
    "# inp_te = scaler_S.transform(inp_te) \n",
    "\n",
    "# # printouts for double checking all the sets and amounts\n",
    "# print('Sizes of Datasets : Inputs , Targets')\n",
    "# print('------------------------------------')\n",
    "# print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "# print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimate(estimator, X, Y, n_splits=200):\n",
    "                          \n",
    "    scoresA = []\n",
    "    scoresP = []\n",
    "    scoresR = []\n",
    "    \n",
    "    for n in range(0,n_splits):\n",
    "        inp_tr, inp_va, tar_tr, tar_va = train_test_split(X,Y) \n",
    "        scaler_S = StandardScaler().fit(inp_tr)\n",
    "        inp_tr = scaler_S.transform(inp_tr)\n",
    "        inp_va = scaler_S.transform(inp_va)\n",
    "        estimator.fit(inp_tr, tar_tr.ravel())  \n",
    "        pred_va = estimator.predict(inp_va)\n",
    "        scoresA.append(accuracy_score(tar_va,pred_va))\n",
    "        scoresR.append(recall_score(tar_va,pred_va,average=None,zero_division=1))  \n",
    "        scoresP.append(precision_score(tar_va,pred_va,average=None,zero_division=1)) \n",
    "    scoresR = list(map(list, zip(*scoresR)))\n",
    "    scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "    estimateA = np.mean(scoresA)*100.\n",
    "    stderrA = np.std(scoresA)*100.\n",
    "    \n",
    "    estimateR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.]\n",
    "    stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.]\n",
    "    \n",
    "    estimateP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.]\n",
    "    stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.]\n",
    "    \n",
    "    return estimateR, stderrR, estimateP, stderrP, estimateA, stderrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"PRAScores_ArtificialBalance.txt\",\"w\")\n",
    "f.write(\"Artificially balanced dataset provided by Hossen Teimoorinia \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM4fxaVCQ2I"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lr2hNedVCQ2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = LogisticRegression('l1',max_iter=500,solver='saga',tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Contaminants\"]\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(logreg, X, Y, n_splits=200)\n",
    "\n",
    "\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56H3DN1ICQ2K"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1MOXdkACQ2K",
    "outputId": "d055072c-5552-4a57-e7ac-cf340a67e0ee"
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf',gamma='auto',C=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onNpzooWWLlX",
    "outputId": "1f817af8-35c4-4d65-d019-e2c97dfb9146"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(svc, X, Y, n_splits=200)\n",
    "\n",
    "f.write(\"LR Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z4DZPwFTZEP"
   },
   "source": [
    "## SVM/LR Stacking Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "wd6LgbdVTduX",
    "outputId": "8013ddeb-ef74-41c4-9494-13282be076ad"
   },
   "outputs": [],
   "source": [
    "estimators = [('svc', SVC(kernel='rbf',gamma='auto',C=0.9,random_state=42))]\n",
    "stacl = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(penalty = 'l1', max_iter = 500, solver ='saga', tol =0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ynA7OC3hUlHg",
    "outputId": "fd2b0ccb-429b-46d2-e85b-5ad69ca93359"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(stacl, X, Y, n_splits=200)\n",
    "\n",
    "f.write(\"Stack Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6A0mcoACQ2N"
   },
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzcB_s2NCQ2L"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "L6GloAZcCQ2M",
    "outputId": "bbe6b011-4640-4876-906f-e232a1fb5754"
   },
   "outputs": [],
   "source": [
    "boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=7,max_features='log2',\n",
    "                n_estimators=100,n_iter_no_change=5,subsample=1.0,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GId2kQ7_CQ2M",
    "outputId": "efb6aba3-eb49-4e4b-b0b0-a60629ddd6df"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(boostcl, X, Y, n_splits=200)\n",
    "\n",
    "f.write(\"GB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v379VtHtCQ2N"
   },
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bI4UnZIuCQ2N",
    "outputId": "d2456ea3-a4e4-4b4e-beb7-a4565c0de316"
   },
   "outputs": [],
   "source": [
    "xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NjT1jfOuS-c7",
    "outputId": "1c76e70a-d7c7-4e67-e646-7adecbc107b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(xgbcl, X, Y, n_splits=200)\n",
    "\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "n4qZO8L_CQ2O"
   },
   "outputs": [],
   "source": [
    "# Final hyperparameters\n",
    "rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=100,oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KHU1_UQ2CQ2O",
    "outputId": "a1a4039b-c207-41ea-cc8c-b6e2b2d09feb"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(rfcl, X, Y, n_splits=200)\n",
    "\n",
    "f.write(\"RF Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hxFSJaxIhcvZ",
    "outputId": "a34455c2-ef18-4c6e-f3c8-00821f87bdf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system('say \"tadaaa your program has probably failed\"')\n",
    "os.system('say \"beep\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLJGW1U6CQ2O",
    "tags": []
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qp5HGBdhcvZ"
   },
   "source": [
    "In this section, we describe in which ways we replicated the CM21 paper, and the choices we made along the way to derive the best results we were able to achieve. This section was performed by S. Fielder.\n",
    "\n",
    "All the training has been performed prior to this notebook, and the state of the network saved at the appropriate time. Below we will just import the state of the system described, and run the testing set through for evaluation, and metrics along with Confusion Matrixes will be outputted for each subsection.\n",
    "\n",
    "Many of the functions called below are semi-custom made, and are found in the appropriate `.py` files in this directory. The majority come from `NN_Defs.py` where the model construction along with the training and validation functions are located. Additionally, our custom data split maker is found in `custom_dataloader.py`, in which we can build reproducable sets of training, validation and testing sets depending on the number of subclasses wanted in each set. In this example, we focus on the results achieved by using the CM21 data-split as performed using this loader.\n",
    "\n",
    "Finally `network_runner.py` was the script used in order to train all of the networks, and appropriately print out the metrics, along with saving plots for both confusion matrixes and loss values. Some of these outputs are shown in the `Saved_Final_Data/` directory. The settings found therein will be imported below to load the state of the networks as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdxoY6vtCQ2P"
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nfkaxzihCQ2P",
    "outputId": "d861bc81-c67d-4342-fec0-282577a69705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on : cpu\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "\n",
    "# custom script inputs\n",
    "from NN_Defs import get_n_params, train, validate, BaseMLP\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on : {device}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7a87715bbc43d8b5f73b6200b6ef66f163e7bfd9f5c97aea1eada326c99da2f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
