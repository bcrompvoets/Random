{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GLopVOsCQ2A",
    "tags": []
   },
   "source": [
    "# Testing ML techniques to identify YSOs in Spitzer IRAC data\n",
    "\n",
    "## Breanna Crompvoets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq3kOX1MH-wS",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Libraries and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oNZdspOvCQ2C"
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classic ML libraries\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data\n",
    "custom_labs = ['Class 1', 'Class 2', 'Others']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RJ3awIPCQ2D",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Classical ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-unH4thCQ2F"
   },
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uaJUpxFCQ2G",
    "outputId": "e9df8de9-9b57-477a-d9c0-9fe62741f2d4"
   },
   "outputs": [],
   "source": [
    "# data load\n",
    "X = np.load(\"../inp.npy\")\n",
    "Y = np.load(\"../tar.npy\")\n",
    "input_te = np.load(\"../inp_test.npy\")\n",
    "target_te = np.load(\"../tar_test.npy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimate(estimator, X, Y, inp_te, tar_te, n_splits=200):\n",
    "                          \n",
    "    scoresA = []\n",
    "    scoresP = []\n",
    "    scoresR = []\n",
    "    \n",
    "    for n in range(0,n_splits):\n",
    "        inp_tr, inp_va, tar_tr, tar_va = train_test_split(X,Y) \n",
    "        scaler_S = StandardScaler().fit(inp_tr)\n",
    "        inp_tr = scaler_S.transform(inp_tr)\n",
    "        inp_va = scaler_S.transform(inp_va)\n",
    "        inp_te = scaler_S.transform(inp_te)\n",
    "        estimator.fit(inp_tr, tar_tr.ravel())  \n",
    "        pred_te = estimator.predict(inp_te)\n",
    "        scoresA.append(accuracy_score(tar_te,pred_te))\n",
    "        scoresR.append(recall_score(tar_te,pred_te,average=None,zero_division=1))  \n",
    "        scoresP.append(precision_score(tar_te,pred_te,average=None,zero_division=1)) \n",
    "    scoresR = list(map(list, zip(*scoresR)))\n",
    "    scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "    estimateA = np.mean(scoresA)*100.\n",
    "    stderrA = np.std(scoresA)*100.\n",
    "    \n",
    "    estimateR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.]\n",
    "    stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.]\n",
    "    \n",
    "    estimateP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.]\n",
    "    stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.]\n",
    "    \n",
    "    return estimateR, stderrR, estimateP, stderrP, estimateA, stderrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"PRAScores_ArtificialBalance_TestSet.txt\",\"w\")\n",
    "f.write(\"Artificially balanced dataset provided by Hossen Teimoorinia \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM4fxaVCQ2I"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Lr2hNedVCQ2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = LogisticRegression('l1',max_iter=500,solver='saga',tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Contaminants\"]\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(logreg, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56H3DN1ICQ2K"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1MOXdkACQ2K",
    "outputId": "d055072c-5552-4a57-e7ac-cf340a67e0ee"
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf',gamma='auto',C=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onNpzooWWLlX",
    "outputId": "1f817af8-35c4-4d65-d019-e2c97dfb9146"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(svc, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "f.write(\"LR Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z4DZPwFTZEP"
   },
   "source": [
    "## SVM/LR Stacking Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "wd6LgbdVTduX",
    "outputId": "8013ddeb-ef74-41c4-9494-13282be076ad"
   },
   "outputs": [],
   "source": [
    "estimators = [('svc', SVC(kernel='rbf',gamma='auto',C=0.9,random_state=42))]\n",
    "stacl = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(penalty = 'l1', max_iter = 500, solver ='saga', tol =0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ynA7OC3hUlHg",
    "outputId": "fd2b0ccb-429b-46d2-e85b-5ad69ca93359"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(stacl, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "f.write(\"Stack Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6A0mcoACQ2N"
   },
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4qZO8L_CQ2O"
   },
   "outputs": [],
   "source": [
    "# Final hyperparameters\n",
    "rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=100,oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHU1_UQ2CQ2O",
    "outputId": "a1a4039b-c207-41ea-cc8c-b6e2b2d09feb"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(rfcl, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "f.write(\"RF Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzcB_s2NCQ2L"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "L6GloAZcCQ2M",
    "outputId": "bbe6b011-4640-4876-906f-e232a1fb5754"
   },
   "outputs": [],
   "source": [
    "boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=7,max_features='log2',\n",
    "                n_estimators=100,n_iter_no_change=5,subsample=1.0,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GId2kQ7_CQ2M",
    "outputId": "efb6aba3-eb49-4e4b-b0b0-a60629ddd6df"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(boostcl, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "f.write(\"GB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v379VtHtCQ2N"
   },
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI4UnZIuCQ2N",
    "outputId": "d2456ea3-a4e4-4b4e-beb7-a4565c0de316"
   },
   "outputs": [],
   "source": [
    "xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjT1jfOuS-c7",
    "outputId": "1c76e70a-d7c7-4e67-e646-7adecbc107b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "estR, stderrR, estP, stderrP, estA, stderrA = bootstrap_estimate(xgbcl, X, Y, input_te, target_te, n_splits=200)\n",
    "\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==1:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxFSJaxIhcvZ",
    "outputId": "a34455c2-ef18-4c6e-f3c8-00821f87bdf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system('say \"tadaaa your program has probably failed\"')\n",
    "os.system('say \"beep\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLJGW1U6CQ2O",
    "tags": []
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdxoY6vtCQ2P"
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nfkaxzihCQ2P",
    "outputId": "d861bc81-c67d-4342-fec0-282577a69705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on : cpu\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "\n",
    "# custom script inputs\n",
    "from NN_Defs import get_n_params, train, validate, BaseMLP\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on : {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def bootstrap(NN,epochs):\n",
    "    train_loader, val_loader = MLP_data_setup(X, Y)\n",
    "    pred_va, tar_va = main(epochs,NN,optimizer,train_loader,val_loader)\n",
    "    ScoresA = accuracy_score(tar_va,pred_va)\n",
    "    ScoresR = recall_score(tar_va,pred_va,average=None,zero_division=1)\n",
    "    ScoresP = precision_score(tar_va,pred_va,average=None,zero_division=1)\n",
    "\n",
    "    return ScoresR, ScoresP, ScoresA\n",
    "\n",
    "\n",
    "\n",
    "def main(epochs, NetInstance, OptInstance, train_loader, val_loader, ScheduleInstance=None):\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss, train_predictions, train_truth_values = train(epoch, NetInstance, OptInstance, train_loader, device)\n",
    "        val_loss, val_accuracy, val_predictions, val_truth_values = validate(NetInstance, val_loader, device)\n",
    "        \n",
    "        if ScheduleInstance is not None:\n",
    "            ScheduleInstance.step()\n",
    "\n",
    "    return val_predictions, val_truth_values\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_data_setup(X,Y):\n",
    "    inp_tr, inp_va, tar_tr, tar_va = train_test_split(X,Y)\n",
    "    \n",
    "    # scaling data according to training inputs\n",
    "    scaler_S = StandardScaler().fit(inp_tr)\n",
    "    inp_tr = scaler_S.transform(inp_tr)\n",
    "    inp_va = scaler_S.transform(inp_va)\n",
    "\n",
    "    # creation of tensor instances\n",
    "\n",
    "    inp_tr = torch.as_tensor(inp_tr)\n",
    "    tar_tr = torch.as_tensor(tar_tr)\n",
    "    inp_va = torch.as_tensor(inp_va)\n",
    "    tar_va = torch.as_tensor(tar_va)\n",
    "\n",
    "    # pass tensors into TensorDataset instances\n",
    "    train_data = data_utils.TensorDataset(inp_tr, tar_tr)\n",
    "    val_data = data_utils.TensorDataset(inp_va, tar_va)\n",
    "\n",
    "    # constructing data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=25, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=25, shuffle=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "X = np.load(\"../inp.npy\")\n",
    "Y = np.load(\"../tar.npy\") \n",
    "\n",
    "\n",
    "BaseNN = BaseMLP(4, 20, 3, weight_initialize=True)\n",
    "## load settings in\n",
    "optimizer = optim.SGD(BaseNN.parameters(), lr=4e-2, momentum=0.9)\n",
    "train_loader, val_loader = MLP_data_setup(X, Y)\n",
    "pred_va, tar_va = main(1000,BaseNN,optimizer,train_loader,val_loader)\n",
    "print(accuracy_score(tar_va,pred_va))\n",
    "print(recall_score(tar_va,pred_va,average=None,zero_division=1))\n",
    "print(precision_score(tar_va,pred_va,average=None,zero_division=1))\n",
    "\n",
    "\n",
    "# estR, stderrR, estP, stderrP, estA, stderrA  = bootstrap_estimate_MLP(BaseNN, X, Y, n_splits=200, epochs =50000)\n",
    "# iters = [(BaseNN,50000)] * 50\n",
    "# ans = []\n",
    "# for n in [0,1,2,3]:\n",
    "\n",
    "#     with mp.Pool(12) as pool:\n",
    "#         ans.append(pool.starmap(bootstrap, iters))\n",
    "#     np.save(f\"intermediatesave_onelayerMLP_{n}.npy\",ans)\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7a87715bbc43d8b5f73b6200b6ef66f163e7bfd9f5c97aea1eada326c99da2f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
