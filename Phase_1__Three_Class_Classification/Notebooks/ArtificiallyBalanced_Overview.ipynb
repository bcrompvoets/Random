{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GLopVOsCQ2A",
    "tags": []
   },
   "source": [
    "# Project on testing ML techniques to identify YSOs in Spitzer IRAC data\n",
    "\n",
    "## Breanna Crompvoets and Samuel Fielder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r3MvlnrCQ2B",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Project Summary and Goals\n",
    "Young Stellar Objects (YSOs) are newly forming stars which are yet to begin burning. They are split into different classes depending on their dust/gas envelope to protostar ratio; Class 0 having the greatest envelope and Class III no longer having an envelope. Due to the different ratios of envelope to protostar, each class appears differently in spectroscopy; thus the difference in fluxes between Spitzer IRAC bands are able to determine which class the data comes from. This project will focus on using the same data as Cornu and Montillaud (2021; CM21) to classify data points into three classes: Class I, Class II, and Contaminants. Only these three classes out of the original 9 available are chosen as Class 0 is too dusty to detect, and Class III is difficult to distinguish from regular stars. Furthermore, the contaminating classes (galaxies, shocks, stars, and PAHs) are of less concern -- we would like the algorithms to focus on distinguishing Class I and Class II from the rest. The original paper uses a multi-layer perceptron (MLP) with one hidden layer (20 neurons). Their results are presented in the below table.\n",
    "\n",
    "|Class | Recall | Precision |\n",
    "| --- | --- | --- |\n",
    "|1 | 94.0% | 79.1 %|\n",
    "|2 | 96.7% | 90.6% | \n",
    "|Other | 98.7%| 99.8%| \n",
    "\n",
    "The data for this project was pulled from https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/647/A116. These data include columns for four Spitzer IRAC bands (3.6 $\\mu m$, 4.5 $\\mu m$, 5.8 $\\mu m$, and 8 $\\mu m$) fluxes and errors, as well as from one Spitzer MIPS band (24 $\\mu m$), along with the target values as determined via a manual classification scheme and the predicted data from CM21. We will only be using the four IRAC bands and their associated errors, as the MIPS band does not provide data for most objects. We use the same target values as they do for accurate comparison. \n",
    "\n",
    "This project seeks to use a multitude of algorithms learned over the semester to measure their effectiveness and compare it to the recreated MLP of CM21. These algorithms include: GridSearch with an SVC, GridSearch with a Logistic Regressor, a Stacking Ensemble with an SVC and a Logistic Regressor, a Gradient Boosting ensemble, a Random Forest ensemble, and an XGBoost ensemble. We also created our own MLP based off of their prescription. The workload was split as follows: B. Crompvoets completed the data cleaning and all other algorithms besides the MLP, and S. Fielder completed an MLP close to that of CM21, as well as creating a custom data loader/split. They communicated together on the best hyper-parameters to test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq3kOX1MH-wS",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Libraries and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oNZdspOvCQ2C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breannacrompvoets/miniforge3/envs/SF/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classic ML libraries\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data\n",
    "custom_labs = ['Class 1', 'Class 2', 'Others']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RJ3awIPCQ2D",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Classical ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-unH4thCQ2F"
   },
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uaJUpxFCQ2G",
    "outputId": "e9df8de9-9b57-477a-d9c0-9fe62741f2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of Datasets : Inputs , Targets\n",
      "------------------------------------\n",
      "Training set: (22500, 4) , (22500,) \n",
      "Validation set: (7500, 4) , (7500,) \n",
      "Testing Set: (5381, 4), (5381,)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "X = np.load(\"../inp.npy\")\n",
    "Y = np.load(\"../tar.npy\") \n",
    "\n",
    "inp_tr, inp_va, tar_tr, tar_va = train_test_split(X,Y)\n",
    "\n",
    "inp_te = np.load(\"../inp_test.npy\")\n",
    "tar_te = np.load(\"../tar_test.npy\")\n",
    "\n",
    "\n",
    "# scaling data according to training inputs\n",
    "scaler_S = StandardScaler().fit(inp_tr)\n",
    "inp_tr = scaler_S.transform(inp_tr)\n",
    "inp_va = scaler_S.transform(inp_va)\n",
    "inp_te = scaler_S.transform(inp_te) \n",
    "\n",
    "# printouts for double checking all the sets and amounts\n",
    "print('Sizes of Datasets : Inputs , Targets')\n",
    "print('------------------------------------')\n",
    "print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimate(estimator, scoring_func=None, random_seed=0, n_splits=200):\n",
    "                          \n",
    "    scoresA = []\n",
    "    scoresP = []\n",
    "    scoresR = []\n",
    "    \n",
    "    for n in range(0,n_splits):\n",
    "        inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'seven', amounts_train, amounts_val,random.randint(0,1000))\n",
    "        # scaling data according to training inputs\n",
    "        scaler_S = StandardScaler().fit(inp_tr)\n",
    "        inp_tr = scaler_S.transform(inp_tr)\n",
    "        inp_va = scaler_S.transform(inp_va)\n",
    "        estimator.fit(inp_tr, tar_tr.ravel())  \n",
    "        pred_va = estimator.predict(inp_va)\n",
    "        scoresA.append(accuracy_score(tar_va,pred_va))\n",
    "        scoresR.append(recall_score(tar_va,pred_va,average=None,zero_division=1))  \n",
    "        scoresP.append(precision_score(tar_va,pred_va,average=None,zero_division=1)) \n",
    "        print(f'n = {n}') \n",
    "    scoresR = list(map(list, zip(*scoresR)))\n",
    "    scoresP = list(map(list, zip(*scoresP)))\n",
    "\n",
    "    estimateA = np.mean(scoresA)*100.\n",
    "    stderrA = np.std(scoresA)*100.\n",
    "    \n",
    "    estimateR = [np.mean(scoresR[0])*100.,np.mean(scoresR[1])*100.,np.mean(scoresR[2])*100.,np.mean(scoresR[3])*100.,np.mean(scoresR[4])*100.,np.mean(scoresR[5])*100.,np.mean(scoresR[6])*100.]\n",
    "    stderrR = [np.std(scoresR[0])*100.,np.std(scoresR[1])*100.,np.std(scoresR[2])*100.,np.std(scoresR[3])*100.,np.std(scoresR[4])*100.,np.std(scoresR[5])*100.,np.std(scoresR[6])*100.]\n",
    "    \n",
    "    estimateP = [np.mean(scoresP[0])*100.,np.mean(scoresP[1])*100.,np.mean(scoresP[2])*100.,np.mean(scoresP[3])*100.,np.mean(scoresP[4])*100.,np.mean(scoresP[5])*100.,np.mean(scoresP[6])*100.]\n",
    "    stderrP = [np.std(scoresP[0])*100.,np.std(scoresP[1])*100.,np.std(scoresP[2])*100.,np.std(scoresP[3])*100.,np.std(scoresP[4])*100.,np.std(scoresP[5])*100.,np.std(scoresP[6])*100.]\n",
    "    \n",
    "    return estimateR, stderrR, estimateP, stderrP, estimateA, stderrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcl = xgb.XGBClassifier(max_depth=7,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"PRAScores_ArtificialBalance.txt\",\"w\")\n",
    "f.write(\"Artificially balanced dataset provided by Hossen Teimoorinia \\n\")\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM4fxaVCQ2I"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "\n",
    "Specifying logistic regression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "hyperparameters tested over initially\n",
    "param_grid = [{'penalty': ['l1'], 'max_iter': np.arange(300,1500,100),\n",
    "        'solver': ['liblinear', 'saga'], 'tol': np.arange(0.0001,0.01,0.0005)},\n",
    "        {'penalty': ['l2'], 'max_iter': np.arange(300,1500,100),\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'tol': np.arange(0.0001,0.01,0.0005)},\n",
    "        {'penalty': ['elasticnet'], 'max_iter': np.arange(100,2000,100), 'l1_ratio': np.arange(0.1,1.,0.1),\n",
    "        'solver': ['saga'], 'tol': np.arange(0.0001,0.01,0.0005)}]\n",
    "\n",
    "grid = GridSearchCV(logreg,param_grid=param_grid, verbose=1)\n",
    "\n",
    "Run the data through the grid to find optimal results\n",
    "grid.fit(inp_tr, tar_tr.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lr2hNedVCQ2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 12.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = LogisticRegression('l1',max_iter=500,solver='saga',tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Class I\", \"Class II\", \"Contaminants\"]\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "\n",
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(xgbcl, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(xgbcl, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(xgbcl, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "\n",
    "\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56H3DN1ICQ2K"
   },
   "source": [
    "## SVM\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'kernel':['rbf','sigmoid','linear','poly'], 'gamma':['auto','scale'], 'C':np.arange(0.1,1.,0.1)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1MOXdkACQ2K",
    "outputId": "d055072c-5552-4a57-e7ac-cf340a67e0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 µs, sys: 151 µs, total: 274 µs\n",
      "Wall time: 66.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final hyperparameters\n",
    "# 75/25\n",
    "svc = SVC(kernel='rbf',gamma='auto',C=0.9)\n",
    "# 300s\n",
    "# svc = SVC(kernel='linear',gamma='auto',C=0.8)\n",
    "# CM21\n",
    "# svc = SVC(kernel='rbf',gamma='auto',C=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onNpzooWWLlX",
    "outputId": "1f817af8-35c4-4d65-d019-e2c97dfb9146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(svc, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(svc, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(svc, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "f.write(\"LR Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z4DZPwFTZEP"
   },
   "source": [
    "## SVM/LR Stacking Ensemble\n",
    "No GridSearch, uses best pars as defined previously. Adding multiple SVC's does not improve results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "wd6LgbdVTduX",
    "outputId": "8013ddeb-ef74-41c4-9494-13282be076ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 33 µs, total: 61 µs\n",
      "Wall time: 62 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Specify Gradient Boost\n",
    "# 75/25\n",
    "estimators = [('svc', SVC(kernel='rbf',gamma='auto',C=0.9,random_state=42))]\n",
    "# 300s\n",
    "# estimators = [('svc', SVC(C=0.8, gamma='auto', kernel='linear',random_state=42))]\n",
    "# CM21 \n",
    "# estimators = [('svc', SVC(C=0.9, gamma='auto', kernel='rbf',random_state=42))]\n",
    "\n",
    "# As the parameters for the Logistic Regression didn't change much, we use the best pars from the first trial.\n",
    "stacl = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(penalty = 'l1', max_iter = 500, solver ='saga', tol =0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ynA7OC3hUlHg",
    "outputId": "fd2b0ccb-429b-46d2-e85b-5ad69ca93359"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(stacl, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(stacl, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(stacl, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "f.write(\"Stack Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzcB_s2NCQ2L"
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'n_estimators': np.arange(50,250,50),'subsample':[0.5,1.0],\n",
    "              'criterion':['friedman_mse'],'n_iter_no_change':[5],'warm_start':[True,False],\n",
    "              'max_depth':np.arange(1,11,2),'max_features': ['sqrt','log2']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "L6GloAZcCQ2M",
    "outputId": "bbe6b011-4640-4876-906f-e232a1fb5754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 µs, sys: 52 µs, total: 77 µs\n",
      "Wall time: 81.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final hyperparameters\n",
    "# 75/25\n",
    "boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=9,max_features='log2',\n",
    "                n_estimators=50,n_iter_no_change=5,subsample=1.0,warm_start=True)\n",
    "\n",
    "# 300s\n",
    "# boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=5,max_features='log2',\n",
    "#                 n_estimators=150,n_iter_no_change=5,subsample=1.0,warm_start=False)\n",
    "\n",
    "# CM21\n",
    "# boostcl = GradientBoostingClassifier(criterion='friedman_mse',max_depth=7,max_features='log2',\n",
    "#                 n_estimators=200,n_iter_no_change=5,subsample=1.0,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GId2kQ7_CQ2M",
    "outputId": "efb6aba3-eb49-4e4b-b0b0-a60629ddd6df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(boostcl, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(boostcl, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(boostcl, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "f.write(\"GB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v379VtHtCQ2N"
   },
   "source": [
    "### XGBoost\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'subsample':[0.5,1.0],'max_depth':np.arange(1,11,2),'sampling_method':['uniform']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI4UnZIuCQ2N",
    "outputId": "d2456ea3-a4e4-4b4e-beb7-a4565c0de316"
   },
   "outputs": [],
   "source": [
    "# 75/25\n",
    "xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "# 300s\n",
    "# xgbcl = xgb.XGBClassifier(max_depth=7,sampling_method='uniform',subsample=0.5,use_label_encoder=False,eval_metric='mlogloss')\n",
    "# CM21\n",
    "# xgbcl = xgb.XGBClassifier(max_depth=9,sampling_method='uniform',subsample=1.0,use_label_encoder=False,eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjT1jfOuS-c7",
    "outputId": "1c76e70a-d7c7-4e67-e646-7adecbc107b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(xgbcl, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(xgbcl, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(xgbcl, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "f.write(\"XGB Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6A0mcoACQ2N"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Hyperparameters tested over initially\n",
    "param_grid = [{'class_weight': ['balanced_subsample','balanced'], 'n_estimators': np.arange(50,250,50),\n",
    "        'criterion': ['gini', 'entropy'], 'max_features': ['sqrt','log2'], 'oob_score':[True,False]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n4qZO8L_CQ2O"
   },
   "outputs": [],
   "source": [
    "# Final hyperparameters\n",
    "# 75/25\n",
    "rfcl = RandomForestClassifier(class_weight='balanced_subsample',criterion='entropy',max_features='log2',n_estimators=150,oob_score=False)\n",
    "# 300s\n",
    "# rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=50,oob_score=False)\n",
    "# CM21\n",
    "# rfcl = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_features='log2',n_estimators=100,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KHU1_UQ2CQ2O",
    "outputId": "a1a4039b-c207-41ea-cc8c-b6e2b2d09feb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Scores\n",
    "estR, stderrR = bootstrap_estimate(rfcl, scoring_func=recall_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Precision Scores\n",
    "estP, stderrP = bootstrap_estimate(rfcl, scoring_func=precision_score, random_seed=0, n_splits=200)\n",
    "\n",
    "# Accuracy Score\n",
    "estA, stderrA = bootstrap_estimate(rfcl, scoring_func=accuracy_score, random_seed=0, n_splits=200)\n",
    "\n",
    "f.write(\"RF Recall & Precision & Accuracy\\n\")\n",
    "for i, cl in enumerate(classes):\n",
    "    if i==3:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$ & $\"+\"{:.1f}\".format(estA)+\"\\pm\"+\"{:.1f}\".format(stderrA)+\"$ // \\n\")\n",
    "    else:\n",
    "        f.write(cl+\"& $\"+\"{:.1f}\".format(estR[i])+\"\\pm\"+\"{:.1f}\".format(stderrR[i])+\"$ & $\"+\n",
    "            \"{:.1f}\".format(estP[i])+\"\\pm\"+\"{:.1f}\".format(stderrP[i])+\"$&// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hxFSJaxIhcvZ",
    "outputId": "a34455c2-ef18-4c6e-f3c8-00821f87bdf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system('say \"tadaaa your program has probably failed\"')\n",
    "os.system('say \"beep\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLJGW1U6CQ2O",
    "tags": []
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qp5HGBdhcvZ"
   },
   "source": [
    "In this section, we describe in which ways we replicated the CM21 paper, and the choices we made along the way to derive the best results we were able to achieve. This section was performed by S. Fielder.\n",
    "\n",
    "All the training has been performed prior to this notebook, and the state of the network saved at the appropriate time. Below we will just import the state of the system described, and run the testing set through for evaluation, and metrics along with Confusion Matrixes will be outputted for each subsection.\n",
    "\n",
    "Many of the functions called below are semi-custom made, and are found in the appropriate `.py` files in this directory. The majority come from `NN_Defs.py` where the model construction along with the training and validation functions are located. Additionally, our custom data split maker is found in `custom_dataloader.py`, in which we can build reproducable sets of training, validation and testing sets depending on the number of subclasses wanted in each set. In this example, we focus on the results achieved by using the CM21 data-split as performed using this loader.\n",
    "\n",
    "Finally `network_runner.py` was the script used in order to train all of the networks, and appropriately print out the metrics, along with saving plots for both confusion matrixes and loss values. Some of these outputs are shown in the `Saved_Final_Data/` directory. The settings found therein will be imported below to load the state of the networks as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdxoY6vtCQ2P"
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nfkaxzihCQ2P",
    "outputId": "d861bc81-c67d-4342-fec0-282577a69705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on : cpu\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "\n",
    "# custom script inputs\n",
    "from NN_Defs import get_n_params, train, validate, BaseMLP\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on : {device}')\n",
    "\n",
    "datadir = 'Saved_Final_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWSjzREdhcva"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1AKc4LsLhcva",
    "outputId": "9edb5e69-3f92-414c-c400-0c23cde99fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of Datasets : Inputs , Targets\n",
      "------------------------------------\n",
      "Training set: (3586, 8) , (3586,) \n",
      "Validation set: (5377, 8) , (5377,) \n",
      "Testing Set: (17940, 8), (17940,)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# custom made libraries\n",
    "from custom_dataloader import replicate_data\n",
    "# data load\n",
    "X = np.load(\"Input_Class_AllClasses_Sep.npy\")\n",
    "Y = np.load(\"Target_Class_AllClasses_Sep.npy\")\n",
    "# Y = np.load(\"Pred_Class_AllClasses_Sep.npy\") # For predicted targets from CM21\n",
    "\n",
    "# custom data loader to pull in custom sized data set\n",
    "# use seed to get replicable results for now\n",
    "seed_val = 1111\n",
    "\n",
    "# the amounts below are how many of each class of object you want in the training set and validation set - leftover amounts given to testing set\n",
    "\n",
    "\n",
    "# CM21 Split\n",
    "amounts_train = [331,1141,231,529,27,70,1257]\n",
    "amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [331,1141,231+529+27+70+1257] # C-targets\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "# 300s Split\n",
    "# amounts_train = [300,300,300,300,27,70,300]\n",
    "# amounts_val = [82, 531, 104, 278, 6, 17, 4359]\n",
    "# amounts_train = [300,300,300+300+27+70+300]\n",
    "# amounts_val = [82, 531, 104+278+6+17+4359]\n",
    "\n",
    "# # 75/25 Split\n",
    "# amounts_train = [311,1994,391,1043,25,66,21796] #75/25 train\n",
    "# amounts_val = [103,665,130,348,9,22,5449] #75/25 val\n",
    "# amounts_train = [311,1994,391+1043+25+66+21796] #75/25 train\n",
    "# amounts_val = [103,665,130+348+9+22+5449] #75/25 val\n",
    "\n",
    "\n",
    "# calling custom datagrabber here\n",
    "inp_tr, tar_tr, inp_va, tar_va, inp_te, tar_te = replicate_data(X, Y, 'three', amounts_train, amounts_val, seed_val)\n",
    "\n",
    "# scaling data according to training inputs\n",
    "scaler_S = StandardScaler().fit(inp_tr)\n",
    "inp_tr = scaler_S.transform(inp_tr)\n",
    "inp_va = scaler_S.transform(inp_va)\n",
    "inp_te = scaler_S.transform(inp_te) # Comment out for 75/25 split\n",
    "\n",
    "# printouts for double checking all the sets and amounts\n",
    "print('Sizes of Datasets : Inputs , Targets')\n",
    "print('------------------------------------')\n",
    "print(f'Training set: {inp_tr.shape} , {tar_tr.shape} \\nValidation set: {inp_va.shape} , {tar_va.shape} \\nTesting Set: {inp_te.shape}, {tar_te.shape}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXQgSJnOCQ2P"
   },
   "source": [
    "## Scaling, Conversions to Tensors, and DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "y3xYPWWpCQ2Q"
   },
   "outputs": [],
   "source": [
    "# creation of tensor instances\n",
    "\n",
    "inp_tr = torch.tensor(inp_tr)\n",
    "tar_tr = torch.tensor(tar_tr)\n",
    "inp_va = torch.tensor(inp_va)\n",
    "tar_va = torch.tensor(tar_va)\n",
    "# inp_te = torch.tensor(inp_te)\n",
    "# tar_te = torch.tensor(tar_te)\n",
    "\n",
    "# pass tensors into TensorDataset instances\n",
    "train_data = data_utils.TensorDataset(inp_tr, tar_tr)\n",
    "val_data = data_utils.TensorDataset(inp_va, tar_va)\n",
    "# test_data = data_utils.TensorDataset(inp_te, tar_te)\n",
    "\n",
    "# constructing data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=25, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=25, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_estimate_and_ci_MLP(NN, valid_loader, device,  scoring_func=None, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200):\n",
    "                        \n",
    "    scores = []\n",
    "\n",
    "    if scoring_func == accuracy_score:\n",
    "        for n in range(0,n_splits):\n",
    "            val_loss, val_accuracy, val_predictions, val_truth_values = validate(NN, valid_loader, device)\n",
    "            scores.append(scoring_func(val_truth_values,val_predictions))\n",
    "        estimate = np.mean(scores)\n",
    "        lower_bound = np.percentile(scores, 100*(alpha/2))\n",
    "        upper_bound = np.percentile(scores, 100*(1-alpha/2))\n",
    "        stderr = np.std(scores)\n",
    "\n",
    "    else:\n",
    "        for n in range(0,n_splits):\n",
    "            val_loss, val_accuracy, val_predictions, val_truth_values = validate(BaseNN, val_loader, device)\n",
    "            scores.append(scoring_func(val_truth_values,val_predictions,average=None))   \n",
    "            scores = list(map(list, zip(*scores)))\n",
    "    \n",
    "        estimate = [np.mean(scores[0]),np.mean(scores[1]),np.mean(scores[2])]\n",
    "        lower_bound = [np.percentile(scores[0], 100*(alpha/2)),np.percentile(scores[1], 100*(alpha/2)),np.percentile(scores[2], 100*(alpha/2))]\n",
    "        upper_bound = [np.percentile(scores[0], 100*(1-alpha/2)),np.percentile(scores[1], 100*(1-alpha/2)),np.percentile(scores[2], 100*(1-alpha/2))]\n",
    "        stderr = [np.std(scores[0]),np.std(scores[1]),np.std(scores[2])]\n",
    "    \n",
    "    return estimate, lower_bound, upper_bound, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtqRXkQgCQ2Q"
   },
   "source": [
    "## Create Network Instance and Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkuLOdYaCQ2S"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAScores_CM21_C-targets_MLP_2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-targets, CM21 data-split, correct\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m est, low, up, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap_estimate_and_ci_MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaseNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecall_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                              \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP Recall\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(est[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m _\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(low[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}^\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(up[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m} , \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(stderr[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mbootstrap_estimate_and_ci_MLP\u001b[0;34m(NN, valid_loader, device, scoring_func, random_seed, alpha, n_splits)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,n_splits):\n\u001b[0;32m---> 17\u001b[0m         val_loss, val_accuracy, val_predictions, val_truth_values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaseNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(scoring_func(val_truth_values,val_predictions,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))   \n\u001b[1;32m     19\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores)))\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/CM21Report/NN_Defs.py:87\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, device)\u001b[0m\n\u001b[1;32m     84\u001b[0m truth_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# start looping through the batches\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# send to device\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/dataset.py:369\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/StarFormation/lib/python3.10/site-packages/torch/utils/data/dataset.py:369\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create nn instance\n",
    "BaseNN = BaseMLP(8, 20, 3)\n",
    "# load in saved state of network\n",
    "# loadpath = datadir+'Final_CSplit_4e-2_Settings' #CM21 Split\n",
    "loadpath = datadir+'Final_CSplit_CM21Pred_Settings' #CM21 Split - C-tar train\n",
    "# loadpath = datadir+'Final_300sSplit_CM21Pred_Settings' #300 Split - C-tar train\n",
    "# loadpath = datadir+'Final_300s_Settings'\n",
    "# loadpath = datadir+'Final_7525Split_CM21Pred_Settings' # 75/25 Split - Ctar train\n",
    "# loadpath = datadir+'Final_7525Split_Settings' # 75/25 Split\n",
    "\n",
    "BaseNN.load_state_dict(torch.load(loadpath, map_location=device))\n",
    "\n",
    "# compute validation results\n",
    "# val_loss, val_accuracy, val_predictions, val_truth_values = validate(BaseNN, val_loader, device)\n",
    "\n",
    "f = open(\"PRAScores_CM21_C-targets_MLP_2.txt\",\"w\")\n",
    "f.write(\"C-targets, CM21 data-split, correct\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=recall_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "\n",
    "f.write(\"MLP Recall\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\" _{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"} , \"+\"{:.3f}\".format(stderr[0])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\" _{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"} , \"+\"{:.3f}\".format(stderr[1])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\" _{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"} , \"+\"{:.3f}\".format(stderr[2])+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=precision_score, random_seed=0, \n",
    "                              alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"MLP Precision\\n\")\n",
    "f.write(\"{:.3f}\".format(est[0])+\" _{\"+\"{:.3f}\".format(low[0])+\"}^{\"+\"{:.3f}\".format(up[0])+\"} , \"+\"{:.3f}\".format(stderr[0])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[1])+\" _{\"+\"{:.3f}\".format(low[1])+\"}^{\"+\"{:.3f}\".format(up[1])+\"} , \"+\"{:.3f}\".format(stderr[1])+\"//\\n\")\n",
    "f.write(\"{:.3f}\".format(est[2])+\" _{\"+\"{:.3f}\".format(low[2])+\"}^{\"+\"{:.3f}\".format(up[2])+\"} , \"+\"{:.3f}\".format(stderr[2])+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "est, low, up, stderr = bootstrap_estimate_and_ci_MLP(BaseNN, val_loader, device, scoring_func=accuracy_score, random_seed=0, \n",
    "                               alpha=0.05, n_splits=200)\n",
    "                            \n",
    "f.write(\"MLP Accuracy\\n\")\n",
    "f.write(\"{:.3f}\".format(est)+\" _{\"+\"{:.3f}\".format(low)+\"}^{\"+\"{:.3f}\".format(up)+\"} , \"+\"{:.3f}\".format(stderr)+\"//\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcD3yCvfhcvc"
   },
   "source": [
    "Again, the loss plot below was constructed at our last epoch in this specific model loaded. This is loaded in manually here from `/Saved_Final_Data`.\n",
    "\n",
    "<img src=\"Saved_Final_Data/Final_CSplit_4e-2_loss.png\" alt=\"Loss with 4e-5 Learning Rate\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7a87715bbc43d8b5f73b6200b6ef66f163e7bfd9f5c97aea1eada326c99da2f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
